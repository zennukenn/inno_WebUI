version: '3.8'

services:
  # Inno WebUI 单容器服务（前端 + 后端）
  inno-webui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: inno-webui-app
    ports:
      - "80:80"      # Nginx (前端)
      - "8080:8080"  # 后端API (可选，用于直接访问)
    environment:
      - VLLM_API_BASE_URL=${VLLM_API_BASE_URL:-http://localhost:8000/v1}
      - DATABASE_URL=sqlite:///./data/chat.db
      - HOST=0.0.0.0
      - PORT=8080
      - PYTHONPATH=/app
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - ALLOW_DYNAMIC_VLLM_URL=${ALLOW_DYNAMIC_VLLM_URL:-true}
    volumes:
      - app_data:/app/data
      - app_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - inno-webui-network

  # VLLM服务 (可选，需要GPU支持)
  vllm:
    image: vllm/vllm-openai:latest
    container_name: inno-webui-vllm
    ports:
      - "8000:8000"
    volumes:
      - models_data:/models
    command: >
      --model /models/Qwen3-0.6B-GPTQ-Int8
      --host 0.0.0.0
      --port 8000
      --served-model-name Qwen3-0.6B-GPTQ-Int8
      --max-model-len 2048
      --dtype float16
      --quantization gptq
      --gpu-memory-utilization 0.8
      --max-num-seqs 256
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - inno-webui-network
    profiles:
      - gpu  # 只有在使用gpu profile时才启动

volumes:
  app_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data
  app_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./logs
  models_data:
    driver: local

networks:
  inno-webui-network:
    driver: bridge
