version: '3.8'

services:
  # Inno WebUI 单容器服务（前端 + 后端）
  inno-webui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: inno-webui-app
    ports:
      - "80:80"      # Nginx (前端)
      - "8080:8080"  # 后端API (可选，用于直接访问)
    environment:
      - VLLM_API_BASE_URL=${VLLM_API_BASE_URL:-http://vllm:8000/v1}
      - DATABASE_URL=sqlite:///./data/chat.db
      - HOST=0.0.0.0
      - PORT=8080
      - PYTHONPATH=/app
      - NODE_ENV=production
    volumes:
      - app_data:/app/data
      - app_logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # VLLM服务 (可选，需要GPU支持)
  vllm:
    image: vllm/vllm-openai:latest
    container_name: inno-webui-vllm
    ports:
      - "8000:8000"
    volumes:
      - models_data:/models
    command: >
      --model /models/Qwen3-0.6B-GPTQ-Int8
      --host 0.0.0.0
      --port 8000
      --served-model-name Qwen3-0.6B-GPTQ-Int8
      --max-model-len 2048
      --dtype float16
      --quantization gptq
      --gpu-memory-utilization 0.8
      --max-num-seqs 256
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    profiles:
      - gpu  # 只有在使用gpu profile时才启动

volumes:
  app_data:
    driver: local
  app_logs:
    driver: local
  models_data:
    driver: local

networks:
  default:
    name: inno-webui-network
